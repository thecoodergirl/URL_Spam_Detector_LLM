# -*- coding: utf-8 -*-
"""new_dataset_deberta_v3_large_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kSepa_WcSfwI9I_2J5jNMRMSmK5jzTtw
"""

!pip install datasets
!pip install transformers
!pip install accelerate -U
from typing import Optional, Union
import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from dataclasses import dataclass
from transformers import AutoTokenizer
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModel

deberta_v3_large = '/kaggle/input/deberta-v3-large-hf-weights'

"""We begin by loading and processing the train data."""

import pandas as pd

# Read the CSV file into a pandas DataFrame
df_train = pd.read_csv('/content/dataset_phishing.csv')
df_train

"""Let's add another 500 examples to the train set!"""

from transformers import DistilBertTokenizer

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the email texts and prepare the input tensors
tokenized_texts = tokenizer(df_train['url'].tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='pt')

# Assuming you have labels in the 'label' column, convert labels to numerical format
label_mapping = {"legitimate": 0, "phishing": 1}
labels = [label_mapping[label] for label in df_train['status']]

# Create a PyTorch Dataset
from torch.utils.data import Dataset, DataLoader, random_split

class EmailDataset(Dataset):
    def __init__(self, tokenized_texts, labels):
        self.tokenized_texts = tokenized_texts
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.tokenized_texts['input_ids'][idx],
            'attention_mask': self.tokenized_texts['attention_mask'][idx],
            'labels': self.labels[idx]
        }

"""Now that we have gone from 200 -> 700 train examples, let us preprocess the data and begin training."""

# Create the dataset
email_dataset = EmailDataset(tokenized_texts, labels)

# Split the dataset into train and validation sets
train_size = int(0.8 * len(email_dataset))
val_size = len(email_dataset) - train_size
train_dataset, val_dataset = random_split(email_dataset, [train_size, val_size])

"""We first create a HuggingFace `Dataset`."""

# Create DataLoader instances
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

"""And let us now preprocess the examples for training."""

# Import necessary components for transfer learning
from transformers import DistilBertForSequenceClassification, AdamW

# Load the pre-trained DistilBERT model for sequence classification
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Set up the optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)

!pip install accelerate -U

# Set up the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,   # Adjust the number of epochs as needed
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    logging_dir='./logs',
)

# Create the Trainer instance for training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Fine-tune the model on your email dataset
trainer.train()

# Evaluate the model
results = trainer.evaluate()

print(results)  # Print evaluation results

import os
print(os.getcwd())
# Save the trained model
saved_model = model.save_pretrained('./content/')



"""# Predicting on the test set

Now that we have trained our model, let us predict on the test set.
"""

# Load necessary libraries
import pandas as pd
import torch
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer

# Load the test dataset
df_test = pd.read_csv('/content/dataset_phishing.csv')  # Replace with the actual path

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the test email texts
tokenized_test_texts = tokenizer(df_test['url'].tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='pt')

# Load the fine-tuned model from the saved location
model = DistilBertForSequenceClassification.from_pretrained('./content/')

# Put the model in evaluation mode
model.eval()

# Make predictions on the test data
all_predictions = []

with torch.no_grad():
    for idx in range(len(df_test)):
        input_ids = tokenized_test_texts['input_ids'][idx].unsqueeze(0).to(model.device)  # Move input to the appropriate device
        attention_mask = tokenized_test_texts['attention_mask'][idx].unsqueeze(0).to(model.device)  # Move attention mask to the appropriate device

        # Forward pass through the model
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        # Get predicted label (class)
        predicted_label = torch.argmax(outputs.logits, dim=1).item()

        # Convert label to status name ('legitimate' or 'phishing')
        predicted_status = "legitimate" if predicted_label == 0 else "phishing"

        # Store the predicted status
        all_predictions.append(predicted_status)

# Add predicted statuses to the test DataFrame
df_test['predicted_status'] = all_predictions

# Save the DataFrame with predictions
df_test.to_csv('test_predictions.csv', index=False)

import torch
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer

# Load the fine-tuned model from the saved location
model = DistilBertForSequenceClassification.from_pretrained('./content/')

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Put the model in evaluation mode
model.eval()

while True:
    url = input("Enter a URL (or type 'exit' to quit): ")

    if url.lower() == 'exit':
        break

    # Tokenize the input URL and prepare the input tensors
    tokenized_input = tokenizer(url, padding='max_length', truncation=True, max_length=128, return_tensors='pt')
    input_ids = tokenized_input['input_ids'].to(model.device)
    attention_mask = tokenized_input['attention_mask'].to(model.device)

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predicted_label = torch.argmax(outputs.logits, dim=1).item()

    predicted_status = "legitimate" if predicted_label == 0 else "phishing"
    print(f"The predicted status for the URL '{url}' is: {predicted_status}")

!pip install streamlit

# Import necessary libraries
import streamlit as st
import torch
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer

# Load the fine-tuned model from the saved location
model = DistilBertForSequenceClassification.from_pretrained('./content/')

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Put the model in evaluation mode
model.eval()

# Set up the Streamlit app layout
st.title("URL Phishing Detection")
url_input = st.text_input("Enter a URL:", "")
if url_input:
    # Tokenize the input URL and prepare the input tensors
    tokenized_input = tokenizer(url_input, padding='max_length', truncation=True, max_length=128, return_tensors='pt')
    input_ids = tokenized_input['input_ids'].to(model.device)
    attention_mask = tokenized_input['attention_mask'].to(model.device)

    # Forward pass through the model
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predicted_label = torch.argmax(outputs.logits, dim=1).item()

    predicted_status = "legitimate" if predicted_label == 0 else "phishing"

    st.write(f"The predicted status for the URL '{url_input}' is: {predicted_status}")